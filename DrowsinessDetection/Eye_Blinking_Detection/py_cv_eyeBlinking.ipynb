{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eye Blinking Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook by [Prashant Brahmbhatt](https://www.github.com/hashbanger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For detection we have to compute a metric call **Eye Aspect Ratio (EAR)**.  \n",
    "More on this can be found in [this](http://vision.fe.uni-lj.si/cvww2016/proceedings/papers/05.pdf) paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The traditional technique of eye blinking involves methods such as:  \n",
    "- Eye localization.\n",
    "- Thresholding to find the whites of the eyes.\n",
    "- Determining if the “white” region of the eyes disappears for a period of time (indicating a blink).\n",
    "\n",
    "The EAR involves a very simple calculation based on the ratio of distances between facial landmarks of the eyes and it is an efficient as well as fast way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our process involves \n",
    "- perform facial landmark detection \n",
    "- detect blinks in video streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Eye Aspect Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each eye is represented by 6 (x, y) coordinates, starting at the left-corner of the eye and works clockwise from there onwards.  \n",
    "![eye](eye.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the referenced paper, we can take away one equation for EAR:  \n",
    "![ear](ear.png)      \n",
    "\n",
    "The above formula calculates the ratio for the vertical distance to the horizontal distance. The 2 in the denominator is because of the presence of two sets of points for the vertical distance while only one set for the horizontal.   \n",
    "\n",
    "The EAR has almost a constant value while the eye remains open however it decreases rapidly when the eye closes as the vertical distance reaches almost 0, while on opening it again rises almost to the same level which indicates a blink."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting blinks and facial landmarks usign openCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The follwing code is to be written in a script `detect_blinks.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance as dist\n",
    "from imutils.video import FileVideoStream\n",
    "from imutils.video import VideoStream\n",
    "from imutils import face_utils\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import dlib\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to define the `eye_aspect_ratio()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eye_aspect_ratio(eye):\n",
    "    '''Taking the array of eye coordinates and returning the ratio'''\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "    \n",
    "    ear = (A + B)/ (2 * C)\n",
    "    \n",
    "    return ear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we parse our command line arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument('-p','--shape-predictor', required = True,\n",
    "               help = 'path to the facial landmark predictor')\n",
    "ap.add_argument('-v','--video', required = True,\n",
    "               help = 'path to input video file') # omit this for a live video stream\n",
    "args = vars(ap.parse_args())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to set up two constants that one may require tuning as per their requirements. We also require two variables declaration.  \n",
    "- The first constant is for setting the default threshold value.  \n",
    "- The second constant is the number of frames the EAR should be below threshold to consider it a blink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EYE_AR_THRESH = 0.3\n",
    "EYE_AR_CONSEC_FRAMES = 3\n",
    "\n",
    "#initializing the blink counts and frame counters\n",
    "COUNTER = 0\n",
    "TOTAL = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we initialize the dlib face detector and facial landmark detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading the facial landmark predictor ###########')\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(args['shape_predictor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our dlib detector returns all the 68 (x, y) coordinates we need to slice the coordinates for both of our eyes.  \n",
    "We can use the face utils functionality for getting the coordinates for the eyes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "(rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we want to use built in web cam live stream then we would require the line  \n",
    "        `vs = VideoStream(src=0).start()`\n",
    "        \n",
    "For using through Raspberry pi use  \n",
    "        `vs = VideoStream(usePiCamera=True).start()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = FileVideoStream(args[\"video\"]).start()\n",
    "#fileStream = True\n",
    "vs = VideoStream(src=0).start() # for built in camera\n",
    "#vs = VideoStream(usePiCamera=True).start()   # for Raspberry pi module\n",
    "fileStream = False  # use in case of live \n",
    "time.sleep(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write the main code for our script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping over the frames of the video stream\n",
    "while True:\n",
    "    # if this is a file video stream, then we need to check if\n",
    "    # there any more frames left in the buffer to process\n",
    "    if fileStream and not vs.more():\n",
    "        break\n",
    "        \n",
    "    # grab the frame from the threaded video file stream, resize\n",
    "    # it, and convert it to grayscale\n",
    "    frame = vs.read()\n",
    "    frame = imutils.resize(frame, width= 450)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    rects = detector(gray, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting the frame we loop over each of the faces detected. As suggested in the paper, we average out the ratio for eyes and most probably the person will blink both of the eyes together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for rect in rects:\n",
    "        # determining the facial landmarks and then coverting them to a numpy array\n",
    "        shape = predictor(gray, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "        # extract the left and right eye coordinates, then use the\n",
    "        # coordinates to compute the eye aspect ratio for both eyes\n",
    "        leftEye = shape[lStart:lEnd]\n",
    "        rightEye = shape[rStart:rEnd]\n",
    "\n",
    "        leftEAR = eye_aspect_ratio(leftEye)\n",
    "        rightEAR = eye_aspect_ratio(rightEye)\n",
    "\n",
    "        # averaging the eye aspect ratio together for both eyes\n",
    "        ear = (leftEAR + rightEAR) / 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we simple handle the detected facial landmarks for the eye regions.   \n",
    "We compute the convex hull for both the eyes and then visualize the eyes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        leftEyeHull = cv2.convexHull(leftEye)\n",
    "        rightEyeHull = cv2.convexHull(rightEye)\n",
    "        cv2.drawContours(frame, [leftEyeHull], -1, (0, 255, 0), 1 )\n",
    "        cv2.drawContours(frame, [rightEyeHull], -1, (0, 255, 0), 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are yet to determine if the blink happened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        if ear < EYE_AR_THRESH:\n",
    "            COUNTER += 1 # increasing the blink frame counter (requires EYE_AR_CONSEC_FRAMES to be a blink)\n",
    "\n",
    "            else:   # the eyes were closed for sufficient frames then consider it as a blink\n",
    "                if COUNTER >= EAR_AR_CONSEC_FRAMES:\n",
    "                    TOTAL += 1\n",
    "                COUNTER = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we need put the information on the screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        cv2.putText(frame, \"Blinks : {}\".format(TOTAL), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        cv2.putText(frame, \"E.A.R : {}\".format(TOTAL), (300, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        \n",
    "    cv2.imshow(\"frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:**  \n",
    "[www.medium.com ]()  \n",
    "[www.pyimagesearch.com]()    \n",
    "[www.stackoverflow.com]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### de nada!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
