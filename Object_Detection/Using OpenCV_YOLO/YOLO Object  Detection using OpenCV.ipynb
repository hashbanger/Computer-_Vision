{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO Object Detection using OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Prashant Brahmbhatt](https://www.github.com/hashbanger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why CNNs aren't good enough!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, why do we need other image detection algorithms if we already had **Convolutional Neural Networks**?  \n",
    "As you can guess, to overcome the disadvantages of the traditional CNNs, some of them are:  \n",
    "- High computational cost.\n",
    "- If you don't have a good GPU they are quite slow to train (for complex tasks).\n",
    "- They use to need a lot of training data.  \n",
    "- CNNs depend on the initial parameter tuning (for a good point) to avoid local optima.\n",
    "\n",
    "But we do have R-CNNs, Faster R-Cnns as well don't we?  \n",
    "Although, they are much better implemented than vanilla CNNs by using Region Proposal Algorithm which could do localization and convolution classification, they are still quite slow sadly!\n",
    "\n",
    "The CNN are good at image classification that requires a single class associated with an image however in real life scenarios that's not good enough! We require detection of multiple objects in an image and also where are they located, termed as **Object Detection** and **Object Localization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're confused about image classification, object detection, segmentation have a look at this given image.  \n",
    "![img1](img1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The YOLO Approach (You Look Only Once)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the original papers cites, the object detection problem is reframed as a regression problem. YOLO trains on full images and directly optimizes detection performance. It doesn't requires a complex pipeline.  \n",
    "Unlike the sliding window technique it looks at the image only once hence the name. It implcitily encodes textual information about the classes and their appearance. \n",
    "The YOLO sees the entire image at once and gets the entire context of the image and makes rare background errors.   \n",
    "The YOLO is a highly generalizable approach it is less prone to bad performance for unexpected inputs or unknown domains.  \n",
    "\n",
    "### Working\n",
    "- The YOLO divides the image in $S x S$ grid, the if the center of an object lies in a grid then that grid becomes responsible for predicting the class of that object. **(Image 1)**\n",
    "- Each of the grid is responsible of predicting some $B$ bounding boxes and confidence score for those boxes to show how sure the model is about any particular object. The score doesn't indicate what kind of object it is rather if it contains some object. If there is no object then the confidence should be zero (duh!).\n",
    "- Each bounding box is consists of 5 predictions $x,y,w,h$ where the (x,y) are the coordinates of the center of the box relative to the bounds of the cell. The w, h are the width and the height which are predicted relative to the whole image.  \n",
    "- When we visualise all of the predictions we get a bunch of bounding boxes around each object and the thickness of the box depends on the confidence score for that object. **(Image 2)**\n",
    "-  Each grid cell predicts the class probabilities. Given that it's an object, the conditional probabilities for each class of the object.\n",
    "- It predicts only one set of class probabilities per grid cell regardless of $B$. So if the grid predicts a *Dog* that doesn't mean that it contains a Dog but rather if that grid contains an object then most probably it is a dog. **(Image 3)** Then at test time it multiplies multiple conditional class probabilities and the individual box confidence predictions.\n",
    "\n",
    "![img3](img3.png)\n",
    "Where $IOU$ is the ***\"Intersection of Union\"***\n",
    "\n",
    "The output scores not only encodes the probability of the class fitting the box but also how well the box fits the object.\n",
    "\n",
    "- We then have a lot of predictions which can include multiple predictions for the same object by different grids with different threshold values so we use ***Non Max Suppresion***. NMS in a nutshell suppress or discards bounding boxes with confidence score less than a selected threshold and then further discards the ones that are left which do not have maximum values, hence the name. **(Image 4)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img2](img2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection in Images ( Not in real time )\n",
    "\n",
    "\n",
    "Note: The below code will be better implemented using a single py script as the segment execution is not possible due to inclusion of command line arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import time \n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "constructing the argument parse and parse the arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = argparse.ArgumentParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap.add_argument('-i','--image', required = True, help = 'the path to input image')\n",
    "ap.add_argument('-y','--yolo', required = True, help = 'the path to input image')\n",
    "ap.add_argument('-c','--confidence', type = float , default = 0.5, help = 'min probability to filter the weak detections')\n",
    "ap.add_argument('-t','--threshold', type = float , default = 0.3, help = 'threshold to apply in NMS')\n",
    "args = vars(ap.parse_args())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above command line arguments will be processed at runtime and they provide the flexibility of changing the inputs to our script from the terminal.   \n",
    "\n",
    "\n",
    "**--image :** the path to the input image  \n",
    "**--yolo :** base path to th yolo directory  \n",
    "**--confidence :** the minimum probability that will filter out the weaker detections  \n",
    "**--threshold :** the value of the threshold that will be used during teh Non Max Suppression. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we parse the arguments, the args variable becomes a dictionary with the key, value pairs for the command line arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in the next step we will assign random colors to the different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_path = os.path.sep.join([args['yolo'], \"coco.names\"])\n",
    "LABELS = open(labels_path).read().strip().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initializing a list of colors to represent each unique class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(45)\n",
    "COLORS = np.random.randint(0, 255, size = (len(LABELS), 3), dtype= 'uint8') # The second 3 argument is because of RGB values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we derive the paths to the yolo trained weights and the configuration files from our disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightsPath =os.path.sep.join([args['yolo'], \"yolov3.weights\"])\n",
    "configPath = os.path.sep.join([args['yolo'], \"yolov3.cfg\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading our yolo detector trained on the coco dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For loading YOLO from the disk, we’ll take advantage of OpenCV’s DNN function called **cv2.dnn.readNetFromDarknet**.  \n",
    "This function requires both the configPath and weightsPath that we already have as command line arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('loading the model from disk...')\n",
    "net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the image and send it into the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getting the image dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(args['image'])\n",
    "(H, W) = image.shape[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "determining the output layer names that we need from the yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln = net.getLayerNames()\n",
    "ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now construct a blob from the image and then perform a forward pass of the YOLO object detector giving us the bounding boxes and the associated probabilities.  \n",
    "We pass the blob from our model network and show the time taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = cv2.dnn.blobFromImage(image, 1/255.0, (416, 416), swapRB = True, crop = False)\n",
    "net.setInput(blob)\n",
    "start = time.time()\n",
    "layerOutputs = net.forward(ln)\n",
    "end = time.time()  \n",
    "print(\"YOLO took {:.6f} seconds\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to initailize some listst that we will require.  \n",
    "**boxes** - our bounding boxes around the object  \n",
    "**confidences** - our model's confidece values that will show how confident our YOLO is in determining an object.  \n",
    "**classIDs** - the detected object's class label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = []\n",
    "confidences = []\n",
    "classIDs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We populate these lists with our network outputs.  \n",
    "Now we loop over each of the layer Outputs then we loop over each detection in output and extract the classID and the confidence.  \n",
    "We use the confidence to filter out weak detections.\n",
    "\n",
    "After filtering out the unwanted detections we,  \n",
    "- Scale the bounding box coordinates so we can display them properly on our original image.  \n",
    "- Extract coordinates and dimensions of the bounding box. YOLO returns bounding box coordinates in the form: (centerX, centerY, width, and height) .\n",
    "- Use this information to derive the top-left (x, y)-coordinates of the bounding box.\n",
    "- Update the boxes , confidences , and classIDs  lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for output in layerOutputs:\n",
    "    for detection in output:\n",
    "        scores = detection[5:]\n",
    "        classID = np.argmax(scores)\n",
    "        confidence = scores[classID]\n",
    "        \n",
    "        if confidence > args['confidence']:\n",
    "            box = detection[0:4] * np.array([W, H, W, H])\n",
    "            (centerX, centerY, width, height) = box.astype('int')\n",
    "            \n",
    "            x = int(centerX - (width / 2))\n",
    "            y = int(centerY - (height / 2))\n",
    "            \n",
    "            boxes.append([x, y, int(width), int(height)])\n",
    "            confidences.append(float(confidence))\n",
    "            classIDs.append(classID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLO doesn't apply NMS automatically so to suppress the weak detections we apply NMS explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = cv2.dnn.NMSBoxes(boxes, confidences, args['confidence'], arg['threshold'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we draw the boxes ad class text on the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(idxs ) > 0:\n",
    "    for i in idxs.flatten():\n",
    "        (x, y) = (boxes[i][0], boxes[i][1])\n",
    "        (w, h) = (boxes[i][2], boxes[i][3])\n",
    "        \n",
    "        color = [int(c) for c in COLORS[classIDs[i]]]\n",
    "        cv2.rectangle(image, (x, y), (x+w, y+h), color ,2)\n",
    "        text = \"{}: {:.2f}\".format(LABELS[classIDs[i]], confidences[i])\n",
    "        cv2.putText(image, text, (x, y -5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color ,2)\n",
    "\n",
    "        \n",
    "cv2.imshow(\"Image\", image)\n",
    "cv2.waitkey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute, go to the base path where the script is located and then shell like:  \n",
    "**python yolo.py --image images/image.jpg --yolo yolo-coco**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "references:    \n",
    "https://arxiv.org/pdf/1506.02640v5.pdf (The original Paper)  \n",
    "https://www.pjreddie.com  \n",
    "https://www.stackoverflow.com  \n",
    "https://www.medium.com  \n",
    "https://www.pyimagesearch.com      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### de nada!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
